| задача | команды                                               | комментарий |
| ------ | ----------------------------------------------------- | ----------- |
|        | gsutil -m cp gs://polyakovspbaquet/taxi_0000000000* . |             |

В качестве одной из СУБД была выбрана MS SQL Server 2019 (Dev). Датасет taxi_trips был разбит на csv и файлы общим объёмом в 11,9Гб был подгружен на Postgres и MS SQL. На Postgres была проблема, что тяжеловато даётся переход на Linux, выгрузка проходила под учёткой postgres, у которой не было прав на каталог /tmp/ в котором хранились файлы с данными. Сама загрузка длилась 10 минут 12 секунд. На MS SQL всё было несколько печальнее, пришлось писать процедуру, т.к. загрузка csv из каталога из коробки отсутствует. Сама загрузка данных на MS SQL заняла 52 минуты 34 секунды. Пробовал дать нагрузку, запросив все данные из полученной таблице на Postgres и MS SQL. Ситуация получилась странная, мелкие запросы, до 70000 строк на Postgres выполнялись сильно быстрее (в несколько раз, один из примеров 400+/- мс на Postgres против 3 секунд на MS SQL). Но вот на полном наборе процесс postgres просто убивался самой ОС, а на 100000 postgres повисал. 

Закрадываются сомнения конечно в правдивости теста, т.к. интересуясь вопросом производительности, я находил информацию, что MS SQL всё же быстрее. Но у меня в качестве сервера MS выступал ноут с 2мя ядрами и 4Гб и SSD, который по скорости не сильно быстрее обычных свежих HDD. 

Так же возникает вопрос полезности таких процедур. Если загрузка из файла - разовая операция, то ищутся мануалы в сети и быстренько выполняются на сервере. Если же такая необходимость возникает регулярно - значит, вероятно, что-то в проекте не так, мне в данном случае кажется логичнее реализовывать какой-то сервис, по средствам которого будут передаваться например json с набором данных, а он уже будет парситься и результат парсинга вставляться в таблицу через обычные запросы. 
